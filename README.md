# Data Modeling with PostgreSQL

## Overview

- This porject is a part of [Become a Data Engineer Nanodegree](https://www.udacity.com/course/data-engineer-nanodegree--nd027) at [Udacity](https://www.udacity.com)

- The project is about building a Data Warehouse using AWS RedShift

## Dataset

### Song Dataset

- The first dataset is a subset of real data from the [Million](https://labrosa.ee.columbia.edu/millionsong/) Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

> song_data/A/B/C/TRABCEI128F424C983.json</br>
> song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```json
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Dataset

- The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

> song_data/A/B/C/TRABCEI128F424C983.json</br>
> song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

![log data image](log-data.png)

### Schema for Song Play Analysis

Using the song and log datasets, created a star schema optimized for queries on song play analysis. This includes the following tables.

#### Fact Table

**songplays** - records in log data associated with song plays i.e. records with page NextSong

- songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### Dimension Tables

**users** - users in the app

- user_id, first_name, last_name, gender, level

**songs** - songs in music database

- song_id, title, artist_id, year, duration

**artists** - artists in music database

- artist_id, name, location, latitude, longitude

**time** - timestamps of records in songplays broken down into specific units

- start_time, hour, day, week, month, year, weekday

#### Design Choice

I chosed for my diminsion tables to distribute style all to replicate the tables in all clusters</br>
exept for timestamp as it will grow as my fact table grow</br>

### ETL process

In order to acheive this database schema we followed a seriese of steps to reach the above schema:

1. We created 2 staging tables for events_log_data and songs_data

2. We created the tables we need with the schema as above

3. We connected to the S3 where we have the data and copied it into the staging tables

4. Then we loaded the data from the staging tables to our final tables

### How to run

Simply in your prefared CLI (command line interface) excute the following:

1. To create or reset the database and the tables `python create_tables.py`

2. To run the etl process `python etl.py`

And for testing you can query the tables in AWS RedShift cluster
